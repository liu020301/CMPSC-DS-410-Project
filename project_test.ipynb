{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reference: Some codes from CMPSC/DS 410 Lab6 are used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructField, StructType, StringType, LongType, IntegerType, FloatType, DateType,BooleanType\n",
    "from pyspark.sql.functions import col, column, avg, when, udf\n",
    "from pyspark.sql.functions import expr\n",
    "from pyspark.sql.functions import split\n",
    "from pyspark.sql import Row\n",
    "from pyspark.mllib.recommendation import ALS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Purpose: This is for finding the best training parameter with k = [4, 7, 10, 13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss=SparkSession.builder.appName(\"ALS-based Recommendation Systems\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ss.sparkContext.getConf().getAll())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss.sparkContext.setCheckpointDir(\"~/scratch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rating_schema = StructType([ StructField(\"app_id\", IntegerType(), True), \\\n",
    "                            StructField(\"helpful\", IntegerType(), True ), \\\n",
    "                            StructField(\"funny\", IntegerType(), True ), \\\n",
    "                            StructField(\"date\", DateType(), True ), \\\n",
    "                            StructField(\"is_recommended\", BooleanType(), True ), \\\n",
    "                            StructField(\"hours\", FloatType(), True ), \\\n",
    "                            StructField(\"user_id\", IntegerType(), True ), \\\n",
    "                            StructField(\"review_id\", IntegerType(), True ), \\\n",
    "                           ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the recommendations.csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF = ss.read.csv(\"/storage/home/zql5426/work/Project/recommendations.csv\", schema=rating_schema, header=True, inferSchema=False)\n",
    "DF = DF.select(\"app_id\", \"is_recommended\", \"hours\", \"user_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate the average hours of each game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_hours_df = DF.groupBy(\"app_id\").agg(avg(\"hours\").alias(\"avg_hours\"))\n",
    "df_with_avghr = DF.join(avg_hours_df, \"app_id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rating according to is_recommended and average hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_points(is_recommended, hours, avg_hours):\n",
    "    if is_recommended and hours >= avg_hours:\n",
    "        return 4\n",
    "    elif is_recommended and hours < avg_hours:\n",
    "        return 3\n",
    "    elif not is_recommended and hours > avg_hours:\n",
    "        return 2\n",
    "    elif not is_recommended and hours < avg_hours:\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assign_points_udf = udf(assign_points, IntegerType())\n",
    "df_with_points = df_with_avghr.withColumn(\"rating_point\", assign_points_udf(col(\"is_recommended\"), col(\"hours\"), col(\"avg_hours\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_df = df_with_points.select(\"user_id\", \"app_id\", \"rating_point\")\n",
    "ratings_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_df = ratings_df.withColumn(\"rating_points\", ratings_df[\"rating_point\"].cast(\"float\"))\n",
    "info_df = ratings_df.select(\"user_id\", \"app_id\", \"rating_points\")\n",
    "info_df.printSchema()\n",
    "info_df = info_df.na.drop(subset=[\"rating_points\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info_RDD = info_df.rdd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split 'info_RDD' into three groups: 60% training, 20% validation, and 20% testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_RDD, validation_RDD, test_RDD = info_RDD.randomSplit([3, 1, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare input (UserID, MovieID) for training, validation and for testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_input_RDD = training_RDD.map(lambda x: (x[0], x[1]) )\n",
    "validation_input_RDD = validation_RDD.map(lambda x: (x[0], x[1]) ) \n",
    "testing_input_RDD = test_RDD.map(lambda x: (x[0], x[1]) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loop through different hyper-parameter settings to find the best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Initialize a Pandas DataFrame to store evaluation results of all combination of hyper-parameter settings\n",
    "hyperparams_eval_df = pd.DataFrame( columns = ['k', 'regularization', 'iterations', 'validation RMS', 'testing RMS'] )\n",
    "# initialize index to the hyperparam_eval_df to 0\n",
    "index =0 \n",
    "# initialize lowest_error\n",
    "lowest_validation_error = float('inf')\n",
    "# Set up the possible hyperparameter values to be evaluated\n",
    "iterations_list = [15, 30]\n",
    "regularization_list = [0.1, 0.2, 0.3]\n",
    "rank_list = [4]\n",
    "for k in rank_list:\n",
    "    for regularization in regularization_list:\n",
    "        for iterations in iterations_list:\n",
    "            seed = 37\n",
    "            # Construct a recommendation model using a set of hyper-parameter values and training data\n",
    "            model = ALS.train(training_RDD, k, seed=seed, iterations=iterations, lambda_=regularization)\n",
    "            # Evaluate the model using evalution data\n",
    "            # map the output into ( (userID, movieID), rating ) so that we can join with actual evaluation data\n",
    "            # using (userID, movieID) as keys.\n",
    "            validation_prediction_RDD= model.predictAll(validation_input_RDD).map(lambda x: ( (x[0], x[1]), x[2] ) )\n",
    "            validation_evaluation_RDD = validation_RDD.map(lambda y: ( (y[0], y[1]), y[2] ) ).join(validation_prediction_RDD)\n",
    "            # Calculate RMS error between the actual rating and predicted rating for (userID, movieID) pairs in validation dataset\n",
    "            validation_error = math.sqrt(validation_evaluation_RDD.map(lambda z: (z[1][0] - z[1][1])**2).mean())\n",
    "            # Save the error as a row in a pandas DataFrame\n",
    "            hyperparams_eval_df.loc[index] = [k, regularization, iterations, validation_error, float('inf')]\n",
    "            index = index + 1\n",
    "            # Check whether the current error is the lowest\n",
    "            if validation_error < lowest_validation_error:\n",
    "                best_k = k\n",
    "                best_regularization = regularization\n",
    "                best_iterations = iterations\n",
    "                best_index = index - 1\n",
    "                lowest_validation_error = validation_error\n",
    "print('The best rank k is ', best_k, ', regularization = ', best_regularization, ', iterations = ',\\\n",
    "      best_iterations, '. Validation Error =', lowest_validation_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ds410_sp24)",
   "language": "python",
   "name": "ds410_sp24"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
